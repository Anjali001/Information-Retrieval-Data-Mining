# -*- coding: utf-8 -*-
"""IRDM_cw2-Part2_FInal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15r2cCOEsabSNJKtVLlKSAcubfLRf_fSq
"""

import pandas as pd
import numpy as np
import string
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
pd.options.mode.chained_assignment = None
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove_file = '/content/drive/MyDrive/IRDM Coursework 2/glove.6B.50d.txt'
tmp_file = get_tmpfile("test_word2vec.txt")
_ = glove2word2vec(glove_file, tmp_file)
model = KeyedVectors.load_word2vec_format(tmp_file,binary=False)

def preprocess_single_passage(passage,stop_words=True):
    tokenizer = RegexpTokenizer(r'\w+')
    tok_pass = tokenizer.tokenize(passage)
    tok_pass = [tok for tok in tok_pass if tok.isalpha()]
    tok_pass = [tok.lower() for tok in tok_pass]
    
    if stop_words == True:
        stop_words = stopwords.words('english')
        tokens = [tok for tok in tok_pass if tok not in stop_words]
    else:
        tokens = tok_pass
    return tokens

validation_data = pd.read_csv('/content/drive/MyDrive/IRDM Coursework 2/validation_data.tsv',sep='\t')
print(validation_data.shape)

train_data = pd.read_csv('/content/drive/MyDrive/IRDM Coursework 2/train_data.tsv',sep='\t')
print(train_data.shape)

def negative_sampling(data,k):
  qid_list = np.unique(np.asarray(data['qid']))
  samples = []
  for qid in qid_list:
    pos_temp = data[(data['qid'] == qid) & (data['relevancy'] == 1)]
    neg_temp = data[(data['qid'] == qid) & (data['relevancy'] == 0)]
    samples.append(pos_temp.sample(n=1, random_state=1))
    if len(neg_temp) < k:
      samples.append(neg_temp)
    else:
      samples.append(neg_temp.sample(n=k, random_state=1))
  new_data = pd.concat(samples)
  return new_data.reset_index(drop=True)

final_train = negative_sampling(train_data,10)
final_train = final_train.reset_index(drop=True)

#preprocess queries and turn queries into list of tokens

# query_tokens_dict = {}
# qid_list, ind_list = np.unique(np.asarray(validation_data['qid']),return_index=True)
# for qid, ind in tqdm(zip(qid_list,ind_list)):
#   query_tokens_dict[qid] = preprocess_single_passage(validation_data.loc[ind,'queries'])
# validation_data.loc[:,'query_tokens'] = validation_data['qid'].map(query_tokens_dict)

# passage_tokens_dict = {}
# pid_list, ind_list = np.unique(np.asarray(validation_data['pid']),return_index=True)
# for pid, ind in tqdm(zip(pid_list,ind_list)):
#   passage_tokens_dict[pid] = preprocess_single_passage(validation_data.loc[ind,'passage'])
# validation_data['passage_tokens'] = validation_data['pid'].map(passage_tokens_dict)

#preprocess queries and turn queries into list of tokens

# query_tokens_dict = {}
# qid_list, ind_list = np.unique(np.asarray(final_train['qid']),return_index=True)
# for qid, ind in tqdm(zip(qid_list,ind_list)):
#   query_tokens_dict[qid] = preprocess_single_passage(final_train.loc[ind,'queries'])
# final_train.loc[:,'query_tokens'] = final_train['qid'].map(query_tokens_dict)

# passage_tokens_dict = {}
# pid_list, ind_list = np.unique(np.asarray(final_train['pid']),return_index=True)
# for pid, ind in tqdm(zip(pid_list,ind_list)):
#   passage_tokens_dict[pid] = preprocess_single_passage(final_train.loc[ind,'passage'])
# final_train['passage_tokens'] = final_train['pid'].map(passage_tokens_dict)

final_train.iloc[9952]

final_train['passage_tokens'] = 0
for i,passage in enumerate(tqdm(final_train['passage'])):
  final_train['passage_tokens'][i] = preprocess_single_passage(passage)

final_train['query_tokens'] = 0
for i,query in enumerate(tqdm(final_train['queries'])):
  final_train['query_tokens'][i] = preprocess_single_passage(query)

validation_data['passage_tokens'] = 0
for i,passage in enumerate(tqdm(validation_data['passage'])):
  validation_data['passage_tokens'][i] = preprocess_single_passage(passage)

validation_data['query_tokens'] = 0
for i,query in enumerate(tqdm(validation_data['queries'])):
  validation_data['query_tokens'][i] = preprocess_single_passage(query)

def get_embedding(tokens):
  '''
  INPUT
  tokens: a list of tokens
  OUTPUT
  average embedding: a vectorb represents average embedding of the input list of tokens
  '''
  embedding = 0
  nom = len(tokens)
  for token in tokens:
    if token not in model:
      nom -= 1
    else:
      embedding += model[token]
  try:
    return embedding/nom
  except:
    return 0

passage_embedding_dict = {}
pid_list, ind_list = np.unique(np.asarray(final_train['pid']),return_index=True)
for pid,ind in tqdm(zip(pid_list,ind_list)):
  passage_embedding_dict[pid] = get_embedding(final_train.loc[ind,'passage_tokens']) 
final_train['passage_embedding'] = final_train['pid'].map(passage_embedding_dict)

query_embedding_dict = {}
qid_list, ind_list = np.unique(np.asarray(final_train['qid']),return_index=True)
for qid,ind in tqdm(zip(qid_list,ind_list)):
  query_embedding_dict[qid] = get_embedding(final_train.loc[ind,'query_tokens']) 
final_train['query_embedding'] = final_train['qid'].map(query_embedding_dict)

passage_embedding_dict = {}
pid_list, ind_list = np.unique(np.asarray(validation_data['pid']),return_index=True)
for pid,ind in tqdm(zip(pid_list,ind_list)):
  passage_embedding_dict[pid] = get_embedding(validation_data.loc[ind,'passage_tokens']) 
validation_data['passage_embedding'] = validation_data['pid'].map(passage_embedding_dict)

query_embedding_dict = {}
qid_list, ind_list = np.unique(np.asarray(validation_data['qid']),return_index=True)
for qid,ind in tqdm(zip(qid_list,ind_list)):
  query_embedding_dict[qid] = get_embedding(validation_data.loc[ind,'query_tokens']) 
validation_data['query_embedding'] = validation_data['qid'].map(query_embedding_dict)

# validation_data['passage_embedding'] = np.empty((len(validation_data), 0)).tolist()
# sum_em = 0
# for i,tkn in enumerate(tqdm(validation_data['passage_tokens'])):
#   for t in tkn:
#     v = 0
#     if t not in model:
#       v+=1
#       continue
#     else:
#       sum_em += model[t]
#   validation_data['passage_embedding'][i] = [x/(len(tkn)-v) for x in sum_em]

# def pid_emb(pid):
#   em=0
#   sum_em = 0
#   tkns = validation_data['passage_tokens'][pid]
#   for tkn in tkns:
#     v = 0
#     if tkn not in model:
#       v+=1
#       continue
#     else:
#       sum_em += model[tkn]
#     em = [x/(len(tkn)-v) for x in sum_em]
#   return em

# key_list = np.unique(validation_data['pid'])
# passage_emb_dict = {}
# for i,key in enumerate(tqdm(key_list)):
#   passage_emb_dict[key] = pid_emb(key)

# new_unique_qid = validation_data.drop_duplicates(subset=['qid'])

# new_unique_qid = new_unique_qid.set_index('qid')

# def qid_emb(qid):
#   em=0
#   sum_em = 0
#   tkns = validation_data['query_tokens'][qid]
#   for tkn in tkns:
#     v = 0
#     if tkn not in model:
#       v+=1
#       continue
#     else:
#       sum_em += model[tkn]
#     em = [x/(len(tkn)-v) for x in sum_em]
#   return em

# key_list = np.unique(validation_data['qid'])
# query_emb_dict = {}
# for i,key in enumerate(tqdm(key_list)):
#   query_emb_dict[key] = qid_emb(key)

# validation_data['query_embedding'] = validation_data['qid'].map(query_emb_dict)

def cosine_similarity(data):
  temp = []
  for i in tqdm(range(len(data))):
    denom = np.dot(data.loc[i,'query_embedding'],data.loc[i,'passage_embedding'])
    nom = np.sqrt(np.square(data.loc[i,'query_embedding']).sum())*np.sqrt(np.square(data.loc[i,'passage_embedding']).sum())
    if nom == 0:
      temp.append(0)
    else:
      temp.append(denom/nom)
  data['cosine_similarity'] = temp
  return data

cosine_similarity(validation_data)

validation_data.isnull().sum()

cosine_similarity(final_train)

final_train['cosine_similarity'].isnull().sum()

validation_data['cosine_similarity'].isnull().sum()

validation_data.to_csv('validation_data_cosine.csv')
!cp validation_data_cosine.csv "/content/drive/MyDrive/IRDM Coursework 2"

final_train.to_csv('train_data_cosine.csv')
!cp train_data_cosine.csv "/content/drive/MyDrive/IRDM Coursework 2"

"""LOGISTIC"""

final_train = pd.read_csv('/content/drive/MyDrive/IRDM Coursework 2/train_data_cosine.csv',index_col=0)

final_train.head()

validation_data = pd.read_csv('/content/drive/MyDrive/IRDM Coursework 2/validation_data_cosine.csv',index_col=0)

validation_data.head()

final_train['DocLen'] = 0
final_train['queryLen'] = 0
for i, row in final_train.iterrows():
  final_train['DocLen'][i] = len(final_train['passage'][i])
  final_train['queryLen'][i] = len(final_train['queries'][i])

validation_data['DocLen'] = 0
validation_data['queryLen'] = 0
for i, row in validation_data.iterrows():
  validation_data['DocLen'][i] = len(validation_data['passage'][i])
  validation_data['queryLen'][i] = len(validation_data['queries'][i])

xTr = final_train[['cosine_similarity','DocLen','queryLen']]
# xTr = final_train[['cosine_similarity']]
yTr = final_train['relevancy']
xTe = validation_data[['cosine_similarity','DocLen','queryLen']]
# xTe = validation_data[['cosine_similarity']]
yTe = validation_data['relevancy']

class LogisticReg:
  def __init__(self,lr,n_iters):
    self.lr = lr
    self.n_iters = n_iters
    self.weights = None
    self.bias = None
  
  def fit(self,X,y):
    n,d = X.shape
    self.weights = np.ones(d)
    self.bias = 0 #np.zeros(n)

    #gradient descent 
    for _ in range(self.n_iters):
      linear = np.dot(X,self.weights) + self.bias #xw+b
      y_predicted = self.sigmoid(linear) #sigmoid
      #caluclate  gradients for bias and weights
      dw = (1/n) * np.dot(X.T,(y_predicted-y)) #2 is scaling
      db = (1/n) * np.sum(y_predicted-y)
      #update them
      self.weights -= self.lr * dw
      self.bias -= self.lr * db

  def predict(self,X):
    linear = np.dot(X,self.weights) + self.bias
    y_predicted = self.sigmoid(linear)
    class_pred = [1 if i>=0.5 else 0 for i in y_predicted]
    return class_pred, y_predicted

  def sigmoid(self,z):
    return 1/(1+np.exp(-z))

regressor = LogisticReg(lr = 0.1,n_iters = 1000)

regressor.fit(xTr,yTr)

y_pred,prob = regressor.predict(xTe)

train_pred,prob2 = regressor.predict(xTr)

from sklearn.metrics import accuracy_score

accuracy_score(yTr, train_pred)

regressor.weights

from sklearn.metrics import accuracy_score
accuracy_score(yTe, y_pred)

regressor1 = LogisticReg(lr = 0.01,n_iters = 1000)
regressor1.fit(xTr,yTr)
y_pred1,prob1 = regressor1.predict(xTe)
accuracy_score(yTe, y_pred1)

regressor2 = LogisticReg(lr = 0.001,n_iters = 1000)
regressor2.fit(xTr,yTr)
y_pred2,prob2 = regressor2.predict(xTe)
accuracy_score(yTe, y_pred2)

regressor3 = LogisticReg(lr = 0,n_iters = 1000)
regressor3.fit(xTr,yTr)
y_pred3,prob3 = regressor3.predict(xTe)
accuracy_score(yTe, y_pred3)

regressor4 = LogisticReg(lr = 1,n_iters = 1000)
regressor4.fit(xTr,yTr)
y_pred4,prob4 = regressor4.predict(xTe)
accuracy_score(yTe, y_pred4)

validation_data['LogitRegr_prob'] = prob2
validation_data['LogitRegr_label'] = y_pred2

#For every qid rank results. So each rank comes almost 1148 times as its unique query length
validation_data['Logit_rank'] = validation_data.groupby('qid')['LogitRegr_prob'].rank(method='first',ascending=False).astype('int')

trial_data = validation_data[['qid','pid','Logit_rank','LogitRegr_prob']]

trial_data.head()

LR_dict = {}
qid_list = trial_data['qid'].unique()
for qid in qid_list:
    top_ones = trial_data[trial_data['qid'] == qid]
    top_ones = top_ones.reset_index(drop=True)
    top_ones = top_ones.sort_values(by=['Logit_rank'])
    LR_dict[qid] = top_ones[:100]
    # top_100 = top_ones.sort_values(by=top_ones['Logit_rank'])
    # LR_dict[qid] = top_100

f = open("LR.txt", "w")
for lr_df in LR_dict.values():
    for i, data in lr_df.iterrows():
        qid = str(data['qid'].astype(int))
        pid = str(data['pid'].astype(int))
        score = str(data['LogitRegr_prob'])
        rank = str(data['Logit_rank'].astype(int))
        f.write(qid + "," + "A2" + "," + pid + "," + rank + "," + score + "," + "LR" + "\n")
f.close()

def average_precision_calc(df,retrieved,score,rank):
    average_precision = 0
    qid_list = np.unique(np.asarray(df['qid']))
    ranked_passages = df[df[rank] <= retrieved]

    relevant_passage = ranked_passages[ranked_passages['relevancy'] != 0]
    relevant_passage['rank'] = relevant_passage.groupby('qid')[score].rank(method = 'first',ascending=False)

    for qid in qid_list:
        temp = relevant_passage[relevant_passage['qid'] == qid]
        temp['rank'] = temp['rank']/temp[rank]
        if len(temp) == 0:
            average_precision += 0
        else:
            average_precision += sum(temp['rank'])/len(temp)

    average_precision = average_precision/len(qid_list)
    return average_precision

average_precision_LR = average_precision_calc(validation_data,100,'LogitRegr_prob','Logit_rank')

average_precision_LR

def NDCG_calc(df,retrieved, rank):

    all_DCG = 0
    relevant_passage = df[df['relevancy'] != 0]
    relevant_passage_retrived = relevant_passage[relevant_passage[rank] <= retrieved]

    qid_list = np.unique(np.asarray(df['qid']))

    for qid in qid_list:
        temp = relevant_passage[relevant_passage['qid'] == qid]
        DCG = sum(1/np.log2(np.asarray(temp[rank])+1))
        optDCG = sum(1/np.log2(np.arange(1,len(temp)+1)+1))
        all_DCG += DCG/optDCG
    all_DCG = all_DCG/len(qid_list)

    return all_DCG

NDCG_LR = NDCG_calc(validation_data,100,'Logit_rank')

NDCG_LR