# -*- coding: utf-8 -*-
"""IRDM_cw2-Part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19FJsGurViwtQRFKgQ4iSgwK1oTHIklRK
"""

import pandas as pd
import numpy as np
import string
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

def preprocess_single_passage(passage,stop_words=True):
    tokenizer = RegexpTokenizer(r'\w+')
    passage = passage.lower()
    tok_pass = tokenizer.tokenize(passage)
    tok_pass = [tok for tok in tok_pass if tok.isalpha()]
    if stop_words == True:
        stop_words = stopwords.words('english')
        tokens = [tok for tok in tok_pass if tok not in stop_words]
    else:
        tokens = tok_pass
    return tokens

candidate_passages_all = pd.read_csv('candidate_passages_top1000.tsv',sep='\t',names=['qid','pid','query','passage'])
candidate_passages_unique = candidate_passages_all.drop_duplicates(subset=['pid'], inplace=False)
N = len(candidate_passages_unique)

candidate_passages_all.head()

test_queries = pd.read_csv('test-queries.tsv',sep='\t',names=['qid','query'])
test_queries.head()

validation_data = pd.read_csv('validation_data.tsv',sep='\t')
validation_data_unique = validation_data.drop_duplicates(subset=['pid'], inplace=False)
print(validation_data.shape)
print(validation_data_unique.shape)

validation_data_unique.head()

inverted_index = {}

for index, data in validation_data_unique.iterrows():
    pid = data['pid']
    tokens = preprocess_single_passage(data['passage'],stop_words=True)
    freq_tokens = nltk.FreqDist(tokens)
    words_passage = len(tokens)
    for token, freq in freq_tokens.items():
        inverted_index.setdefault(token, [])
        inverted_index[token].append((pid, freq, words_passage))

vocab = list(inverted_index.keys())
total_length_vocab = len(vocab)

word_occur_corpus = 0
for idx, data in validation_data_unique.iterrows():
    word_occur_corpus += len(preprocess_single_passage(data['passage']))
avg_passage_len = word_occur_corpus/N

k1 = 1.2
k2 = 100
b = 0.75
R = 0
r = 0

def BM25_model(query, passage):
    q_tokens = preprocess_single_passage(query)
    p_tokens = preprocess_single_passage(passage)
    q_length = len(q_tokens)
    query_freq_dist = nltk.FreqDist(q_tokens)
    passage_freq_dist = nltk.FreqDist(p_tokens)
    doclen = len(p_tokens)
    K = k1*((1-b) + b *(float(doclen)/float(avg_passage_len)))
    score = 0
    for token in q_tokens:
        try:
            n = len(inverted_index[token])
        except:
            n = 0
        f = passage_freq_dist[token]
        qf = query_freq_dist[token]
        one = np.log(((r + 0.5)/(R - r + 0.5))/((n-r+0.5)/(N-n-R+r+0.5)))
        two = ((k1 + 1) * f)/(K+f)
        three = ((k2+1) * qf)/(k2+qf)
        score += one * two * three
    return score

bm25_dict = {}
for qid in np.unique(validation_data['qid']):
    bm25_dict[qid] = []
    validation_data_ = validation_data[validation_data['qid'] == qid]
    for idx2, row2 in validation_data_.iterrows():
        passage = row2['passage']
        query = row2['queries']
        bm25_dict[qid].append(BM25_model(query, passage))

def sim_rank(cosine_sim_results):
    result = np.array(cosine_sim_results).argsort()[-1103039:][::-1]
    return result

results_bm25 = {}
for idx3, row3 in bm25_dict.items():
    qid = idx3   
    results_bm25[qid] = sim_rank(bm25_dict[qid])

# results_bm25 : For every qid I have 100 pid ranked, keys=qid,values=list of pid index

def average_precision_calc(df):
    df = df.reset_index(drop=True, inplace=False)
    R = 0
    avg_precision_num = 0
    for idx4, row4 in df.iterrows():
        relevancy = row4['relevancy']
        if (relevancy):
            R += 1
            avg_precision_num += (R / (idx4 + 1))
    if R==0:
        return 0
    else:
        return (avg_precision_num / R)

def NDCG_calc(df):
  df1 = df.sort_values(by=['relevancy'], ascending=False) #For Perfect NDCG possible - all 1 top and then if 0
  df1 = df1.reset_index().reindex(df1.columns, axis=1) #reset index
  df2 = df.reset_index(drop=True, inplace=False) #For Original DCG
  PDCG = 0
  DCG = 0
  for idx6,row6 in df1.iterrows():
    relevance_score = row6['relevancy']
    gain = 2**(relevance_score)-1
    disc_gain = gain / np.log2(idx6 + 2) #1 more than formula for index
    PDCG += disc_gain

  for idx7,row7 in df2.iterrows():
    relevance_score1 = row7['relevancy']
    gain1 = 2**(relevance_score1)-1
    disc_gain1 = gain1 / np.log2(idx7 + 2)
    DCG += disc_gain1
  
  if PDCG == 0:
    return 0
  else:
    return (DCG/PDCG)

mean_average_precision = 0
mean_NDCG = 0
for qid, indices in results_bm25.items():
    df_qid = validation_data[validation_data['qid'] == qid]
    ranked_df = df_qid.iloc[indices]

    average_precision = average_precision_calc(ranked_df)
    ndcg = NDCG_calc(ranked_df)

    mean_average_precision += average_precision
    mean_NDCG += ndcg

mean_average_precision = mean_average_precision / len(results_bm25)
mean_NDCG = mean_NDCG/len(results_bm25)

print(f'Average Precision is {mean_average_precision}')
print(f'Normalized Discounted Cumulative Gain(NDCG) is {mean_NDCG}')











!wget -P /root/input/ -c "https://nlp.stanford.edu/data/glove.6B.zip"

!unzip /root/input/glove.6B.zip

from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove_file = '/content/glove.6B.50d.txt'
tmp_file = get_tmpfile("test_word2vec.txt")
_ = glove2word2vec(glove_file, tmp_file)
model = KeyedVectors.load_word2vec_format(tmp_file)

model[',']

validation_data.head()